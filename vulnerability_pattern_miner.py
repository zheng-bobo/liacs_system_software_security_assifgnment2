"""
Vulnerability Pattern Mining Tool

Extracts vulnerable code snippets from the MoreFixes database, identifies recurring vulnerability patterns, and generates GitHub search queries.

Main functionalities:
1. Data Extraction: Extracts high-quality vulnerability fix samples from the database (score >= 65, non-empty diff, excludes merge commits)
2. Pattern Identification: Uses CodeSimilarityMatcher for multi-level code similarity matching to identify recurring vulnerability patterns, including:
   - Code normalization (whitespace, identifier normalization)
   - Token shingle generation
   - AST parsing and hashing
   - Keyword extraction
   - Similarity calculation and clustering
3. Query Generation: Generates a GitHub search query for each recognized pattern to discover additional similar vulnerabilities

Module structure:
- code_similarity_matcher.py: Code similarity matching module
- github_query_generator.py: GitHub query generation module

Usage:
    python vulnerability_pattern_miner.py --top-n 3 --min-score 65 --languages java

For details, please refer to README.md
"""

import os
import sys
from pathlib import Path
from typing import Optional, List
import logging
import argparse
from dotenv import load_dotenv
import pandas as pd
import sqlalchemy
from sqlalchemy import text

# 导入拆分的模块
from code_similarity_matcher import CodeSimilarityMatcher
from github_query_generator import GitHubQueryGenerator

# 加载环境变量
load_dotenv(".env")

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("vulnerability_pattern_miner.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


class DatabaseConnector:
    """Database connector"""

    def __init__(self):
        self.engine = None
        self._connect()

    def _connect(self):
        """Connect to the database"""
        try:
            db_url = (
                f'postgresql://{os.getenv("POSTGRES_USER")}:'
                f'{os.getenv("POSTGRES_PASSWORD")}@'
                f'{os.getenv("DB_HOST")}:{os.getenv("POSTGRES_PORT")}/'
                f'{os.getenv("POSTGRES_DB")}'
            )
            self.engine = sqlalchemy.create_engine(db_url)
            logger.info("Database connected successfully")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            sys.exit(1)

    def execute_query(self, query: str, params: Optional[dict] = None) -> pd.DataFrame:
        """Execute query and return DataFrame"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(query), params or {})
                return pd.DataFrame(result.fetchall(), columns=result.keys())
        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            raise


def extract_java_vulnerable_code(
    db_connector: DatabaseConnector,
    min_score: int = 65,
    exclude_merge_commits: bool = True,
    programming_languages: list = None,
    require_diff: bool = True,
) -> pd.DataFrame:
    """
    从数据库中提取指定语言的漏洞代码

    Args:
        db_connector: 数据库连接器
        min_score: fixes.score 的最小值，默认 65 (准确率约在 95%+)
        exclude_merge_commits: 是否排除 merge commit，默认 True
        programming_languages: 编程语言列表，默认 ['Java']
        require_diff: 是否要求 diff 非空，默认 True

    Returns:
        包含漏洞代码信息的 DataFrame
    """
    if programming_languages is None:
        programming_languages = ["Java"]

    logger.info(f"开始提取 {programming_languages} 语言的漏洞代码...")
    logger.info(
        f"筛选条件: min_score={min_score}, exclude_merge={exclude_merge_commits}, require_diff={require_diff}"
    )

    # 构建语言过滤条件（不区分大小写匹配）
    # 允许传入 "java"、"JAVA"、"Java" 等不同大小写的值，都能匹配到数据库中的记录
    lang_conditions = []
    for i, lang in enumerate(programming_languages):
        # 使用 LOWER() 函数进行不区分大小写匹配，使用参数化查询避免 SQL 注入
        lang_conditions.append(f"LOWER(fc.programming_language) = LOWER(:lang_{i})")

    # 准备参数
    params = {"min_score": min_score}
    for i, lang in enumerate(programming_languages):
        params[f"lang_{i}"] = lang

    lang_filter = " OR ".join(lang_conditions)

    # 构建 WHERE 条件
    where_conditions = []

    # diff 条件
    if require_diff:
        where_conditions.append("COALESCE(fc.diff, '') <> ''")

    # merge commit 条件
    if exclude_merge_commits:
        where_conditions.append("COALESCE(c.merge, FALSE) = FALSE")

    # 编程语言条件（不区分大小写）
    where_conditions.append(f"({lang_filter})")

    where_clause = " AND ".join(where_conditions)

    query = f"""
    -- 取"可用于模式挖掘"的高质量修复样本
    WITH good_fixes AS (
      SELECT f.cve_id, f.hash, f.repo_url, f.score
      FROM fixes f
      WHERE f.score >= :min_score
    )
    SELECT
      gf.cve_id,
      gf.repo_url,
      gf.hash,
      gf.score,
      c.author_date,
      c.msg,
      fc.file_change_id,
      fc.filename,
      fc.programming_language,
      fc.code_before,
      fc.code_after,
      fc.diff
    FROM good_fixes gf
    JOIN commits c
      ON c.hash = gf.hash AND c.repo_url = gf.repo_url
    JOIN file_change fc
      ON fc.hash = gf.hash
    WHERE {where_clause};
    """

    df = db_connector.execute_query(query, params=params)

    logger.info(f"提取了 {len(df)} 条漏洞代码记录")
    logger.info(f"涉及 {df['cve_id'].nunique()} 个 CVE")
    logger.info(f"涉及 {df['hash'].nunique()} 个 commit")
    logger.info(f"涉及 {df['repo_url'].nunique()} 个仓库")

    return df


def process_recurring_patterns(
    vulnerable_code_df: pd.DataFrame,
    top_n: int = 3,
    output_dir: Path = None,
    similarity_method: str = "combined",
    similarity_threshold: float = 0.5,
    use_keyword_grouping: bool = True,
) -> pd.DataFrame:
    """
    步骤2: 识别重复漏洞代码模式并保存结果

    使用 CodeSimilarityMatcher 的 find_similar_fixes 方法，结合多种特征识别重复模式，
    并返回 Pattern Record 数据。

    Args:
        vulnerable_code_df: 包含漏洞代码的 DataFrame
        top_n: 返回前 n 个最常见的模式，默认 3
        output_dir: 输出目录，默认 None（使用当前目录下的 output 目录）
        similarity_method: 相似度计算方法，默认 'combined'（综合多特征）
        similarity_threshold: 相似度阈值，默认 0.5
        use_keyword_grouping: 是否使用 keywords 进行预分组以提高效率，默认 True

    Returns:
        包含 Pattern Record 的 DataFrame，包含以下字段：
        - pattern_id: 模式 ID（如 p001）
        - language: 编程语言
        - normalized_pattern_text: 标准化模式文本
        - keyword_tokens: 关键字 tokens 列表
        - regex: 正则表达式模式
        - ast_hash: AST 哈希值
        - example_cves: 示例 CVE 列表
        - example_snippet: 示例代码片段
        - pattern_count: 该模式出现的次数
    """
    logger.info("\n步骤2: 识别重复漏洞代码模式（使用 find_similar_fixes）")

    # 设置输出目录
    if output_dir is None:
        output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    # 使用 CodeSimilarityMatcher 的 find_similar_fixes 方法
    logger.info("使用 CodeSimilarityMatcher.find_similar_fixes 识别重复模式...")
    matcher = CodeSimilarityMatcher(shingle_size=5, use_ast=True)

    # 调用 find_similar_fixes，它会自动创建 Pattern Records
    logger.info(
        f"参数: similarity_method={similarity_method}, similarity_threshold={similarity_threshold}"
    )
    logger.info(f"use_keyword_grouping={use_keyword_grouping}, top_n={top_n}")

    similar_fixes_df, pattern_records_df = matcher.find_similar_fixes(
        vulnerable_code_df,
        top_n=top_n * 10,  # 获取更多相似对以便生成更多模式
        similarity_threshold=similarity_threshold,
        similarity_method=similarity_method,
        use_keyword_grouping=use_keyword_grouping,
        create_patterns=True,
    )

    # 如果没有生成模式记录，返回空 DataFrame
    if pattern_records_df.empty:
        logger.warning("未找到重复模式，返回空结果")
        return pd.DataFrame()

    # 按 pattern_count 降序排序，选择前 top_n 个
    pattern_records_df = pattern_records_df.sort_values(
        "pattern_count", ascending=False
    ).head(top_n)

    logger.info(f"识别出 {len(pattern_records_df)} 个 Pattern Records")
    logger.info(f"返回前 {len(pattern_records_df)} 个最常见的模式")

    # 保存 Pattern Records
    if len(pattern_records_df) > 0:
        # 准备保存的数据（处理列表类型的列）
        patterns_df_to_save = pattern_records_df.copy()

        # 处理列表类型的列
        if "keyword_tokens" in patterns_df_to_save.columns:
            patterns_df_to_save["keyword_tokens"] = patterns_df_to_save[
                "keyword_tokens"
            ].apply(lambda x: ", ".join(x) if isinstance(x, list) else str(x))
        if "example_cves" in patterns_df_to_save.columns:
            patterns_df_to_save["example_cves"] = patterns_df_to_save[
                "example_cves"
            ].apply(lambda x: ", ".join(x) if isinstance(x, list) else str(x))

        # 保存 Pattern Records
        patterns_file = output_dir / f"pattern_records_top{top_n}.csv"
        patterns_df_to_save.to_csv(patterns_file, index=False, encoding="utf-8")
        logger.info(f"Pattern Records 已保存到: {patterns_file}")

        # 保存相似修复对（如果存在）
        if len(similar_fixes_df) > 0:
            similar_fixes_file = output_dir / f"similar_fixes_top{top_n}.csv"
            similar_fixes_df.to_csv(similar_fixes_file, index=False, encoding="utf-8")
            logger.info(f"相似修复对已保存到: {similar_fixes_file}")

        # 打印前几个 Pattern Records 的详细信息
        logger.info("\n" + "=" * 60)
        logger.info(f"前 {min(5, len(pattern_records_df))} 个 Pattern Records:")
        logger.info("=" * 60)
        for idx, (_, row) in enumerate(pattern_records_df.head(5).iterrows(), 1):
            logger.info(f"\nPattern Record #{idx}:")
            if "pattern_id" in row:
                logger.info(f"  Pattern ID: {row['pattern_id']}")
            if "pattern_count" in row:
                logger.info(f"  出现次数: {row['pattern_count']}")
            if "language" in row:
                logger.info(f"  编程语言: {row['language']}")
            if "ast_hash" in row:
                logger.info(f"  AST Hash: {row['ast_hash']}")
            if "keyword_tokens" in row:
                keywords = (
                    row["keyword_tokens"]
                    if isinstance(row["keyword_tokens"], str)
                    else (
                        ", ".join(row["keyword_tokens"])
                        if isinstance(row["keyword_tokens"], list)
                        else ""
                    )
                )
                logger.info(f"  Keywords: {keywords[:200]}...")
            if "example_cves" in row:
                cves = (
                    row["example_cves"]
                    if isinstance(row["example_cves"], str)
                    else (
                        ", ".join(row["example_cves"])
                        if isinstance(row["example_cves"], list)
                        else ""
                    )
                )
                logger.info(f"  示例 CVE: {cves[:200]}...")
            if "normalized_pattern_text" in row:
                logger.info(
                    f"  标准化模式文本 (前100字符): {str(row['normalized_pattern_text'])[:100]}..."
                )
            if "regex" in row:
                logger.info(f"  正则表达式 (前100字符): {str(row['regex'])[:100]}...")
            if "example_snippet" in row:
                logger.info(
                    f"  示例代码片段 (前100字符): {str(row['example_snippet'])[:100]}..."
                )

    return pattern_records_df


def main(
    top_n: int = 3,
    min_score: int = 65,
    exclude_merge_commits: bool = True,
    programming_languages: List[str] = None,
    require_diff: bool = True,
):
    """
    主函数：提取候选重复漏洞代码模式

    Args:
        top_n: 返回出现次数最多的前 n 个模式，默认 3
        min_score: fixes.score 的最小值，默认 65
        exclude_merge_commits: 是否排除 merge commit，默认 True
        programming_languages: 编程语言列表，默认 ['Java']
        require_diff: 是否要求 diff 非空，默认 True
    """
    if programming_languages is None:
        programming_languages = ["Java"]

    logger.info("=" * 60)
    logger.info("开始提取候选重复漏洞代码模式")
    logger.info(f"配置: top_n={top_n}, min_score={min_score}")
    logger.info("=" * 60)

    # 初始化数据库连接
    db_connector = DatabaseConnector()

    # 步骤1: 根据条件筛选漏洞代码
    logger.info("\n步骤1: 提取漏洞代码")
    vulnerable_code_df = extract_java_vulnerable_code(
        db_connector,
        min_score=min_score,
        exclude_merge_commits=exclude_merge_commits,
        programming_languages=programming_languages,
        require_diff=require_diff,
    )

    # 保存原始数据（排除 code_before 和 code_after 列，但包含 score 列）
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    # 准备要保存的列：排除 code_before 和 code_after
    columns_to_save = [
        col
        for col in vulnerable_code_df.columns
        if col not in ["code_before", "code_after"]
    ]
    output_df = vulnerable_code_df[columns_to_save].copy()

    output_file = output_dir / "extract_java_vulnerable_code.csv"
    output_df.to_csv(output_file, index=False, encoding="utf-8")
    logger.info(f"原始数据已保存到: {output_file}")

    # 步骤2: 识别重复模式（使用 CodeSimilarityMatcher）
    recurring_patterns_df = process_recurring_patterns(
        vulnerable_code_df,
        top_n=top_n,
        output_dir=output_dir,
        similarity_method="exact",
    )

    # 步骤3: 生成 GitHub 搜索查询
    if len(recurring_patterns_df) > 0:
        github_queries_df = GitHubQueryGenerator().generate(
            recurring_patterns_df, output_dir=output_dir
        )
        logger.info(
            f"为 {len(recurring_patterns_df)} 个模式生成了 {len(github_queries_df)} 条 GitHub 查询"
        )
    else:
        github_queries_df = pd.DataFrame()
        logger.warning("未找到模式，跳过 GitHub 查询生成")

    # 打印统计信息
    logger.info("\n" + "=" * 60)
    logger.info("统计信息:")
    logger.info(f"  总记录数: {len(vulnerable_code_df)}")
    logger.info(f"  唯一 CVE 数: {vulnerable_code_df['cve_id'].nunique()}")
    logger.info(f"  唯一 commit 数: {vulnerable_code_df['hash'].nunique()}")
    logger.info(f"  唯一仓库数: {vulnerable_code_df['repo_url'].nunique()}")
    logger.info(f"  唯一文件数: {vulnerable_code_df['filename'].nunique()}")
    logger.info(f"  识别出的重复模式数: {len(recurring_patterns_df)}")
    logger.info(f"  生成的 GitHub 查询数: {len(github_queries_df)}")
    logger.info("=" * 60)

    logger.info("\n提取完成！")


def parse_arguments():
    """
    解析命令行参数

    Returns:
        argparse.Namespace: 解析后的命令行参数对象
    """
    parser = argparse.ArgumentParser(description="提取候选重复漏洞代码模式")
    parser.add_argument(
        "--top-n",
        type=int,
        default=3,
        help="返回出现次数最多的前 n 个模式（默认: 3）",
    )
    parser.add_argument(
        "--min-score",
        type=int,
        default=65,
        help="fixes.score 的最小值（默认: 65）",
    )
    parser.add_argument(
        "--include-merge",
        action="store_true",
        help="包含 merge commit（默认: 排除）",
    )
    parser.add_argument(
        "--languages",
        nargs="+",
        default=["java"],  # 可以使用任何大小写形式，如 "java"、"JAVA"、"Java"
        help="编程语言列表，不区分大小写（默认: java）。例如：--languages java 或 --languages Java Go",
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_arguments()

    main(
        top_n=args.top_n,
        min_score=args.min_score,
        exclude_merge_commits=not args.include_merge,
        programming_languages=args.languages,
    )
