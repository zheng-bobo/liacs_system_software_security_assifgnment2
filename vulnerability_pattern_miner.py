"""
Vulnerability Pattern Mining Tool

Extracts vulnerable code snippets from the MoreFixes database, identifies recurring vulnerability patterns
based on CWE types, and generates GitHub search queries.

Main functionalities:
1. Data Extraction: Extracts high-quality vulnerability fix samples from the database
   - Score >= 65 (high accuracy, ~95%+)
   - Non-empty diff required
   - Excludes merge commits
   - Supports Top N CWE filtering

2. Pattern Identification: Uses source/sink/taint analysis to identify recurring vulnerability patterns
   - Source identification: Identifies untrusted input sources (e.g., getParameter, getHeader, readObject)
   - Sink identification: Identifies dangerous usage points (e.g., SQL execution, XSS output, path operations)
   - Taint flow analysis: Tracks data propagation from source to sink
   - Security measures analysis: Identifies missing security measures (e.g., HTML escaping, path normalization)
   - CWE-specific analysis: Supports targeted analysis for specific CWE types:
     * CWE-79: Cross-site Scripting (XSS)
     * CWE-22: Path Traversal
     * NVD-CWE-noinfo: Insufficient Information (uses generic patterns)

3. Query Generation: Generates GitHub search queries for each recognized pattern
   - Generates queries based on source/sink keywords
   - Optimizes keywords according to CWE types
   - Supports GitHub API integration for actual code search
   - Automatically handles GitHub API rate limits

Module structure:
- github_query_generator.py: GitHub query generation and API integration module

Usage:
    python vulnerability_pattern_miner.py --top-n 3 --min-score 65 --languages java

Command-line arguments:
    --top-n: Number of top CWE types to process (default: 3)
    --min-score: Minimum score threshold (default: 65)
    --include-merge: Include merge commits (default: excluded)
    --languages: Programming languages list, case-insensitive (default: java)

Output files:
    - output/extract_java_vulnerable_code.csv: Extracted vulnerable code samples
    - output/top_cwe_top{n}.csv: Top N CWE types list
    - output/cwe_based_patterns_top{n}.csv: Pattern records with GitHub queries
    - output/github_search_results.csv: GitHub API search results (optional)

For details, please refer to README.md
"""

import os
import sys
from pathlib import Path
from typing import Optional, List, Dict, Set, Tuple, Union
import logging
import argparse
from dotenv import load_dotenv
import pandas as pd
import sqlalchemy
from sqlalchemy import text
import re
from collections import defaultdict
import time

# Import split modules
from github_query_generator import GitHubQueryGenerator

# Load environment variables
load_dotenv(".env")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("vulnerability_pattern_miner.log"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

# Java Source patterns configuration (categorized by vulnerability family/use case)
JAVA_SOURCE_PATTERNS = {
    "http": [
        # Servlet API - supports variable names that may not be "request" (req, httpRequest, etc.)
        # Matches \w*request or \w*req, e.g., request, req, httpRequest, servletRequest
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getParameter\s*\(", "getParameter"),
        (
            r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getParameterValues\s*\(",
            "getParameterValues",
        ),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getParameterMap\s*\(", "getParameterMap"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getHeader\s*\(", "getHeader"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getHeaders\s*\(", "getHeaders"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getCookies\s*\(", "getCookies"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getQueryString\s*\(", "getQueryString"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getRequestURI\s*\(", "getRequestURI"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getPathInfo\s*\(", "getPathInfo"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getServletPath\s*\(", "getServletPath"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getAttribute\s*\(", "getAttribute"),
        # HTTP Body reading
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getInputStream\s*\(", "getInputStream"),
        (r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getReader\s*\(", "getReader"),
        # Common library wrappers (e.g., Spring, Apache Commons)
        (
            r"(\w+)\s*=\s*ServletRequestUtils\.getStringParameter\s*\(",
            "ServletRequestUtils.getStringParameter",
        ),
        (r"(\w+)\s*=\s*WebUtils\.getParameter\s*\(", "WebUtils.getParameter"),
        # JAX-RS annotations (simple matching, full support requires AST)
        (r"@PathParam\s*", "PathParam"),
        (r"@QueryParam\s*", "QueryParam"),
        (r"@HeaderParam\s*", "HeaderParam"),
        (r"@FormParam\s*", "FormParam"),
        # Spring Framework annotations
        (r"@RequestParam\s*", "RequestParam"),
        (r"@PathVariable\s*", "PathVariable"),
        (r"@RequestHeader\s*", "RequestHeader"),
        (r"@RequestBody\s*", "RequestBody"),
    ],
    "file": [
        # File path related
        (r"(\w+)\s*=\s*new\s+File\s*\([^)]*(\w+)[^)]*\)", "newFile"),
        (r"(\w+)\s*=\s*Paths\.get\s*\(", "Paths.get"),
        # Get file/path parameters from request (common in file upload, path traversal)
        (
            r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getParameter\s*\(\s*[\"'](file|path|dir|name|filename|upload)[\"']",
            "fileParam",
        ),
        (
            r"(\w+)\s*=\s*\w*[Rr]equest\w*\.getHeader\s*\(\s*[\"'](X-File|X-Path|X-Filename)[\"']",
            "fileHeader",
        ),
    ],
    "deserialization": [
        # Deserialization related
        (r"(\w+)\s*=\s*.*\.readObject\s*\(", "readObject"),
        (r"(\w+)\s*=\s*ObjectInputStream.*\.readObject\s*\(", "readObject"),
        # Create ObjectInputStream from file/HTTP stream (stronger taint source)
        (
            r"ObjectInputStream\s*\(\s*new\s+FileInputStream\s*\(",
            "readObjectFileSource",
        ),
        (
            r"ObjectInputStream\s*\(\s*\w*[Rr]equest\w*\.getInputStream\s*\(",
            "readObjectHttpSource",
        ),
    ],
    "env": [
        # System properties/environment variables
        (r"(\w+)\s*=\s*System\.getenv\s*\(", "getenv"),
        (r"(\w+)\s*=\s*System\.getProperty\s*\(", "getProperty"),
        (r"(\w+)\s*=\s*args\s*\[", "args"),
        # Servlet configuration parameters (lower priority)
        (r"(\w+)\s*=\s*.*\.getInitParameter\s*\(", "getInitParameter"),
    ],
}


class DatabaseConnector:
    """Database connector"""

    def __init__(self):
        self.engine = None
        self._connect()

    def _connect(self):
        """Connect to the database"""
        try:
            db_url = (
                f'postgresql://{os.getenv("POSTGRES_USER")}:'
                f'{os.getenv("POSTGRES_PASSWORD")}@'
                f'{os.getenv("DB_HOST")}:{os.getenv("POSTGRES_PORT")}/'
                f'{os.getenv("POSTGRES_DB")}'
            )
            self.engine = sqlalchemy.create_engine(db_url)
            logger.info("Database connected successfully")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            sys.exit(1)

    def execute_query(self, query: str, params: Optional[dict] = None) -> pd.DataFrame:
        """Execute query and return DataFrame"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(query), params or {})
                # Use fetchall() to get all results, may take some time for large datasets
                rows = result.fetchall()
                df = pd.DataFrame(rows, columns=result.keys())
                return df
        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            raise


def extract_java_vulnerable_code(
    db_connector: DatabaseConnector,
    min_score: int = 65,
    exclude_merge_commits: bool = True,
    programming_languages: list = None,
    require_diff: bool = True,
    top_n: int = None,
    output_dir: Path = None,
) -> pd.DataFrame:
    """
    Extract vulnerable code of specified languages from database, optionally filter top n CWE

    Args:
        db_connector: Database connector
        min_score: Minimum value of fixes.score, default 65 (accuracy ~95%+)
        exclude_merge_commits: Whether to exclude merge commits, default True
        programming_languages: List of programming languages, default ['Java']
        require_diff: Whether to require non-empty diff, default True
        top_n: If specified, only return vulnerable code for top n CWE, default None (return all)
        output_dir: Output directory, default None (use output directory under current directory)

    Returns:
        DataFrame containing vulnerable code information (if top_n is specified, only contains top n CWE data)
    """
    if programming_languages is None:
        programming_languages = ["Java"]

    logger.info(f"Starting to extract vulnerable code for {programming_languages}...")
    logger.info(
        f"Filter conditions: min_score={min_score}, exclude_merge={exclude_merge_commits}, "
        f"require_diff={require_diff}"
    )

    # Build language filter conditions (case-insensitive matching)
    # Allows passing "java", "JAVA", "Java" etc. with different cases, all can match records in database
    lang_conditions = []
    for i, lang in enumerate(programming_languages):
        # Use LOWER() function for case-insensitive matching, use parameterized queries to avoid SQL injection
        lang_conditions.append(f"LOWER(fc.programming_language) = LOWER(:lang_{i})")

    # Prepare parameters
    params = {"min_score": min_score}
    for i, lang in enumerate(programming_languages):
        params[f"lang_{i}"] = lang

    lang_filter = " OR ".join(lang_conditions)

    # Build WHERE conditions
    where_conditions = []

    # diff condition
    if require_diff:
        where_conditions.append("COALESCE(fc.diff, '') <> ''")

    # merge commit condition
    if exclude_merge_commits:
        where_conditions.append("COALESCE(c.merge, FALSE) = FALSE")

    # code_before condition: filter out 'None' strings and NULL values
    where_conditions.append("fc.code_before IS NOT NULL AND fc.code_before <> 'None'")

    # Programming language condition (case-insensitive)
    where_conditions.append(f"({lang_filter})")

    where_clause = " AND ".join(where_conditions)

    query = f"""
    -- Get high-quality fix samples suitable for pattern mining
    WITH good_fixes AS (
      SELECT f.cve_id, f.hash, f.repo_url, f.score
      FROM fixes f
      WHERE f.score >= :min_score
    )
    SELECT
      gf.cve_id,
      gf.repo_url,
      gf.hash,
      gf.score,
      c.author_date,
      c.msg,
      fc.file_change_id,
      fc.filename,
      fc.programming_language,
      fc.code_before,
      fc.code_after,
      fc.diff
    FROM good_fixes gf
    JOIN commits c
      ON c.hash = gf.hash AND c.repo_url = gf.repo_url
    JOIN file_change fc
      ON fc.hash = gf.hash
    WHERE {where_clause};
    """

    logger.info("Executing database query, this may take some time...")
    start_time = time.time()
    df = db_connector.execute_query(query, params=params)
    elapsed_time = time.time() - start_time
    logger.info(f"Database query completed, elapsed time: {elapsed_time:.2f} seconds")

    if df.empty:
        logger.warning("No vulnerable code records extracted")
        return df

    logger.info(f"Extracted {len(df)} vulnerable code records")
    logger.info(f"Involving {df['cve_id'].nunique()} CVEs")
    logger.info(f"Involving {df['hash'].nunique()} commits")
    logger.info(f"Involving {df['repo_url'].nunique()} repositories")

    # If top_n is specified, filter data for top n CWE
    if top_n is not None and top_n > 0:
        logger.info(f"\nFiltering vulnerable code for top {top_n} CWE...")

        # Get CWE classification information
        cwe_classification_df = _get_cwe_classification(df, db_connector)
        if cwe_classification_df.empty:
            logger.warning(
                "No CWE classification information found, returning all data"
            )
            return df

        # Count CVE numbers for each CWE, select top n
        cwe_counts = (
            cwe_classification_df[cwe_classification_df["cve_id"].isin(df["cve_id"])]
            .groupby(["cwe_id", "cwe_name"])
            .size()
            .reset_index(name="fix_count")
            .query("fix_count > 1")
            .sort_values("fix_count", ascending=False)
            .head(top_n)
        )

        if cwe_counts.empty:
            logger.warning("No matching CWE found, returning all data")
            return df

        logger.info(f"Found {len(cwe_counts)} top CWE:")
        for _, row in cwe_counts.iterrows():
            logger.info(
                f"  {row['cwe_id']}: {row['cwe_name']} (fix_count: {row['fix_count']})"
            )

        # Get CVE list for top n CWE
        top_cwe_ids = cwe_counts["cwe_id"].tolist()
        top_cwe_cves = cwe_classification_df[
            cwe_classification_df["cwe_id"].isin(top_cwe_ids)
        ]["cve_id"].unique()

        # Filter df, only keep records belonging to top n CWE
        original_count = len(df)
        df = df[df["cve_id"].isin(top_cwe_cves)]
        filtered_count = len(df)

        logger.info(
            f"Filtering completed: from {original_count} records filtered to {filtered_count} records "
            f"(belonging to top {top_n} CWE)"
        )
        logger.info(f"Involving {df['cve_id'].nunique()} CVEs")
        logger.info(f"Involving {df['hash'].nunique()} commits")

    # Save raw data (exclude code_before and code_after columns, but include score column)
    if output_dir is None:
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)

    # Prepare columns to save: exclude code_before and code_after
    columns_to_save = [
        col for col in df.columns if col not in ["code_before", "code_after"]
    ]
    output_df = df[columns_to_save].copy()

    output_file = output_dir / "extract_java_vulnerable_code.csv"
    output_df.to_csv(output_file, index=False, encoding="utf-8")
    logger.info(f"Raw data saved to: {output_file}")

    return df


def _get_cwe_classification(
    vulnerable_code_df: pd.DataFrame, db_connector: DatabaseConnector
) -> pd.DataFrame:
    """Get CWE classification information for all CVEs in vulnerable_code_df"""
    if vulnerable_code_df.empty or "cve_id" not in vulnerable_code_df.columns:
        return pd.DataFrame()

    unique_cves = vulnerable_code_df["cve_id"].unique()
    if len(unique_cves) == 0:
        return pd.DataFrame()

    placeholders = ", ".join([f":cve_{i}" for i in range(len(unique_cves))])
    params = {f"cve_{i}": cve_id for i, cve_id in enumerate(unique_cves)}

    query = f"""
    SELECT cc.cve_id, cc.cwe_id, c.cwe_name
    FROM cwe_classification cc
    JOIN cwe c ON cc.cwe_id = c.cwe_id
    WHERE cc.cve_id IN ({placeholders});
    """
    return db_connector.execute_query(query, params=params)


def get_top_cwe_by_frequency_from_df(
    vulnerable_code_df: pd.DataFrame,
    db_connector: DatabaseConnector,
    top_n: int = 3,
) -> pd.DataFrame:
    """Based on vulnerable_code_df, count frequency by CWE, select top n CWE by occurrence count"""
    logger.info(
        f"\nStep 2.1: Based on vulnerable_code_df, count frequency by CWE, select top {top_n} CWE"
    )

    cwe_df = _get_cwe_classification(vulnerable_code_df, db_connector)
    if cwe_df.empty:
        logger.warning("No CWE classification information found")
        return pd.DataFrame()

    # Count CVE numbers for each CWE
    cwe_counts = (
        cwe_df[cwe_df["cve_id"].isin(vulnerable_code_df["cve_id"])]
        .groupby(["cwe_id", "cwe_name"])
        .size()
        .reset_index(name="fix_count")
        .query("fix_count > 1")
        .sort_values("fix_count", ascending=False)
        .head(top_n)
    )

    logger.info(f"Found {len(cwe_counts)} top CWE:")
    for _, row in cwe_counts.iterrows():
        logger.info(
            f"  {row['cwe_id']}: {row['cwe_name']} (fix_count: {row['fix_count']})"
        )

    return cwe_counts[["cwe_id", "cwe_name", "fix_count"]]


def get_cves_by_cwe_from_df(
    vulnerable_code_df: pd.DataFrame,
    cwe_id: str,
    cwe_classification_df: pd.DataFrame,
) -> pd.DataFrame:
    """Select data for specified CWE from vulnerable_code_df (includes file_change_id)"""
    logger.info(f"\nStep 2.3: Select {cwe_id} data from vulnerable_code_df")

    # Filter from existing CWE classification
    cwe_cves = cwe_classification_df[cwe_classification_df["cwe_id"] == cwe_id][
        "cve_id"
    ]
    if cwe_cves.empty:
        logger.warning(f"No CVEs found for CWE {cwe_id}")
        return pd.DataFrame()

    # Filter vulnerable_code_df to include data for this CWE (keep all columns, including file_change_id)
    filtered_df = vulnerable_code_df[vulnerable_code_df["cve_id"].isin(cwe_cves)]

    logger.info(
        f"Found {len(filtered_df)} records, involving {filtered_df['cve_id'].nunique()} CVEs"
    )
    return filtered_df


def extract_method_code_from_file(
    file_code: str, start_line: int, end_line: int
) -> str:
    """
    Extract method code from file code based on line number range

    Args:
        file_code: File code
        start_line: Start line number (1-based)
        end_line: End line number (1-based)

    Returns:
        Extracted method code, returns empty string if extraction fails
    """
    if not file_code or not isinstance(file_code, str):
        logger.warning(f"File code is empty or not a string, file_code={file_code}")
        return ""

    try:
        lines = file_code.split("\n")
        # Convert to 0-based index
        start_idx = max(0, start_line - 1)
        end_idx = min(len(lines), end_line)

        if start_idx >= end_idx or start_idx >= len(lines):
            logger.warning(
                f"Invalid line number range, start_idx={start_idx}, end_idx={end_idx}, len(lines)={len(lines)}"
            )
            return ""

        method_code = "\n".join(lines[start_idx:end_idx])
        return method_code.strip()
    except Exception as e:
        logger.warning(f"Failed to extract method code: {e}")
        return ""


def get_method_changes_for_file_changes(
    db_connector: DatabaseConnector,
    file_change_ids: Union[str, List[str]],
    programming_languages: List[str] = None,
) -> pd.DataFrame:
    """
    Step 2.4: Get method-level code changes based on file_change_id (single or list)

    Note: Only returns records where before_change='True' (methods before change, i.e., vulnerable code)

    Args:
        db_connector: Database connector
        file_change_ids: file_change_id (single string) or list of file_change_id
        programming_languages: List of programming languages, default ['Java']

    Returns:
        DataFrame containing method change information, includes the following fields:
        - cve_id: CVE ID
        - file_change_id: File change ID
        - method_change_id: Method change ID
        - method_name: Method name
        - signature: Method signature
        - code_after: Method code after change
        - start_line: Start line number (corresponds to line number in file_code_before)
        - end_line: End line number (corresponds to line number in file_code_before)
        - before_change: Always 'True' (only returns methods before change)
        - file_code_before: File-level code before fix (used to extract method code)
        - file_code_after: File-level code after fix
    """
    if programming_languages is None:
        programming_languages = ["Java"]

    # Unified processing: convert single string to list
    if isinstance(file_change_ids, str):
        file_change_ids = [file_change_ids]

    if not file_change_ids or len(file_change_ids) == 0:
        return pd.DataFrame()

    # Build language filter conditions
    lang_conditions = []
    params = {}
    for i, lang in enumerate(programming_languages):
        lang_conditions.append(f"LOWER(fc.programming_language) = LOWER(:lang_{i})")
        params[f"lang_{i}"] = lang

    lang_filter = " OR ".join(lang_conditions)

    # Choose query method based on number of file_change_id
    if len(file_change_ids) == 1:
        # Single file_change_id, use = condition
        params["file_change_id"] = file_change_ids[0]
        file_change_condition = "fc.file_change_id = :file_change_id"
    else:
        # Multiple file_change_id, use IN condition
        placeholders = ", ".join(
            [f":file_change_{i}" for i in range(len(file_change_ids))]
        )
        for i, file_change_id in enumerate(file_change_ids):
            params[f"file_change_{i}"] = file_change_id
        file_change_condition = f"fc.file_change_id IN ({placeholders})"

    query = f"""
    SELECT DISTINCT
      f.cve_id,
      f.hash,
      f.repo_url,
      fc.file_change_id,
      mc.method_change_id,
      mc.name as method_name,
      mc.signature,
      mc.code as code_after,
      mc.start_line,
      mc.end_line,
      mc.before_change,
      fc.code_before as file_code_before,
      fc.code_after as file_code_after
    FROM file_change fc
    JOIN fixes f ON f.hash = fc.hash
    JOIN method_change mc ON mc.file_change_id = fc.file_change_id
    WHERE {file_change_condition}
      AND ({lang_filter})
      AND fc.code_before IS NOT NULL
      AND fc.code_before <> ''
      AND mc.start_line IS NOT NULL
      AND mc.end_line IS NOT NULL
      AND LOWER(mc.before_change) = 'true'
    ORDER BY fc.file_change_id, mc.start_line;
    """

    df = db_connector.execute_query(query, params=params)

    return df


def _get_source_patterns_for_cwe(cwe_id: str = None) -> List[tuple]:
    """
    Return relevant source patterns based on CWE type

    Args:
        cwe_id: CWE ID

    Returns:
        List of source patterns
    """
    if cwe_id is None:
        # Default: return all patterns
        all_patterns = []
        for category_patterns in JAVA_SOURCE_PATTERNS.values():
            all_patterns.extend(category_patterns)
        return all_patterns

    cwe_id_upper = cwe_id.upper()

    # Select relevant source patterns based on CWE type
    selected_categories = []

    if cwe_id_upper == "CWE-79":  # XSS
        selected_categories = ["http"]
    elif cwe_id_upper == "CWE-89":  # SQL Injection
        selected_categories = ["http"]
    elif cwe_id_upper == "CWE-22":  # Path Traversal
        selected_categories = ["http", "file"]
    elif cwe_id_upper == "CWE-502":  # Deserialization
        selected_categories = ["http", "deserialization"]
    else:
        # For other CWE or NVD-CWE-noinfo, use all patterns
        selected_categories = list(JAVA_SOURCE_PATTERNS.keys())

    # Merge selected categories
    all_patterns = []
    for category in selected_categories:
        if category in JAVA_SOURCE_PATTERNS:
            all_patterns.extend(JAVA_SOURCE_PATTERNS[category])

    return all_patterns


def analyze_source_sink_taint(
    code: str,
    language: str = "java",
    cwe_id: str = None,
) -> Dict:
    """
    Step 2.4: Analyze source, sink, and taint flow in code (optimized for Java)

    Args:
        code: Code to analyze
        language: Programming language, default 'java'
        cwe_id: CWE ID, used to filter relevant source/sink patterns, default None (no filtering)

    Returns:
        Dictionary containing analysis results:
        - sources: List of found sources
        - sinks: List of found sinks
        - taint_flows: List of taint flows
        - tainted_variables: Set of tainted variables
    """
    # Check if code is empty
    if not code or not isinstance(code, str) or code.strip() == "":
        return {
            "sources": [],
            "sinks": [],
            "taint_flows": [],
            "tainted_variables": [],
        }

    if language.lower() != "java":
        logger.warning(
            f"Currently only supports Java language analysis, skipping {language}"
        )
        return {
            "sources": [],
            "sinks": [],
            "taint_flows": [],
            "tainted_variables": [],
        }

    sources = []
    sinks = []
    tainted_variables = set()
    taint_flows = []

    # Get relevant source patterns based on CWE type
    source_patterns = _get_source_patterns_for_cwe(cwe_id)

    # Java Sink patterns (dangerous usage points) - categorized by vulnerability type
    sink_patterns = [
        # SQLi sinks
        (r"\.execute\s*\(", "SQLi", "execute"),
        (r"\.executeQuery\s*\(", "SQLi", "executeQuery"),
        (r"\.executeUpdate\s*\(", "SQLi", "executeUpdate"),
        (r"Statement\s*.*\.execute", "SQLi", "Statement.execute"),
        (r"createStatement\s*\(\)", "SQLi", "createStatement"),
        # XSS sinks (CWE-79)
        (r"\.println\s*\(", "XSS", "println"),
        (r"\.print\s*\(", "XSS", "print"),
        (r"\.write\s*\(", "XSS", "write"),
        (r"\.append\s*\(", "XSS", "append"),
        (r"response\.getWriter\s*\(\)\.", "XSS", "getWriter"),
        (r"out\.print", "XSS", "out.print"),
        (r"PrintWriter.*\.print", "XSS", "PrintWriter.print"),
        (r"response\.setContentType\s*\(", "XSS", "setContentType"),
        (r"document\.write\s*\(", "XSS", "document.write"),
        (r"innerHTML\s*=", "XSS", "innerHTML"),
        (r"outerHTML\s*=", "XSS", "outerHTML"),
        (r"\.html\s*\(", "XSS", "html"),
        # Path traversal sinks (CWE-22)
        (r"new\s+File\s*\(", "PathTraversal", "new File"),
        (r"FileInputStream\s*\(", "PathTraversal", "FileInputStream"),
        (r"FileOutputStream\s*\(", "PathTraversal", "FileOutputStream"),
        (r"Files\.readAllBytes\s*\(", "PathTraversal", "Files.readAllBytes"),
        (r"Files\.readString\s*\(", "PathTraversal", "Files.readString"),
        (r"Files\.copy\s*\(", "PathTraversal", "Files.copy"),
        (r"Files\.move\s*\(", "PathTraversal", "Files.move"),
        (r"Files\.write\s*\(", "PathTraversal", "Files.write"),
        (r"\.getCanonicalPath\s*\(", "PathTraversal", "getCanonicalPath"),
        (r"\.toPath\s*\(", "PathTraversal", "toPath"),
        (r"Paths\.get\s*\(", "PathTraversal", "Paths.get"),
        (r"\.getAbsolutePath\s*\(", "PathTraversal", "getAbsolutePath"),
        (r"\.getAbsoluteFile\s*\(", "PathTraversal", "getAbsoluteFile"),
        # Command injection sinks
        (r"Runtime\.getRuntime\s*\(\)\.exec\s*\(", "CommandInjection", "Runtime.exec"),
        (r"ProcessBuilder\s*\(", "CommandInjection", "ProcessBuilder"),
        # Deserialization sinks
        (r"\.readObject\s*\(", "Deserialization", "readObject"),
        (
            r"ObjectInputStream.*\.readObject",
            "Deserialization",
            "ObjectInputStream.readObject",
        ),
    ]

    # Step 1: Find sources (initialize tainted_variables)
    for pattern_tuple in source_patterns:
        pattern, source_type = (
            pattern_tuple
            if isinstance(pattern_tuple, tuple)
            else (pattern_tuple, "unknown")
        )
        matches = re.finditer(pattern, code, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            # Try to extract variable name (if there is group(1))
            var_name = match.group(1) if match.groups() else None

            # For annotation-type sources (e.g., @RequestParam), may not have variable name
            # Try to extract parameter name from nearby match position (simple heuristic)
            if not var_name and "@" in pattern:
                # For annotations, try to extract parameter name after annotation
                # Example: @RequestParam String paramName
                match_end = match.end()
                next_line = code[
                    match_end : match_end + 100
                ]  # Check next 100 characters
                param_match = re.search(r"@\w+\s+\w+\s+(\w+)", next_line, re.IGNORECASE)
                if param_match:
                    var_name = param_match.group(1)

            if var_name and var_name not in [
                "null",
                "true",
                "false",
            ]:  # Exclude keywords
                sources.append(
                    {
                        "variable": var_name,
                        "pattern": pattern,
                        "type": source_type,
                    }
                )
                tainted_variables.add(var_name)

    # Step 2: Perform multiple rounds of taint propagation (assignment/concatenation/StringBuilder etc.)
    # Example: x = tainted_var; or sql = "SELECT * FROM " + tainted_var; or result = process(tainted_var);
    if tainted_variables:
        # Multiple rounds of iterative propagation until no new tainted variables
        max_iterations = 5
        for iteration in range(max_iterations):
            new_tainted = set()
            escaped_vars = [re.escape(var) for var in tainted_variables]
            var_pattern = "|".join(escaped_vars)

            # Assignment propagation: x = tainted_var
            assignment_pattern = rf"(\w+)\s*=\s*({var_pattern})\s*[;,\)]"
            for match in re.finditer(assignment_pattern, code, re.IGNORECASE):
                new_var = match.group(1)
                if (
                    new_var
                    and new_var not in tainted_variables
                    and new_var not in ["null", "true", "false"]
                ):
                    new_tainted.add(new_var)

            # String concatenation propagation: sql = "SELECT" + tainted_var or sql = tainted_var + "FROM"
            concat_patterns = [
                rf'(\w+)\s*=\s*"[^"]*"\s*\+\s*({var_pattern})',
                rf"(\w+)\s*=\s*({var_pattern})\s*\+\s*\"[^\"]*\"",
                rf"(\w+)\s*=\s*({var_pattern})\s*\+\s*({var_pattern})",  # Concatenation of two tainted variables
            ]
            for pattern in concat_patterns:
                try:
                    for match in re.finditer(pattern, code, re.IGNORECASE):
                        new_var = match.group(1)
                        if new_var and new_var not in tainted_variables:
                            new_tainted.add(new_var)
                except re.error:
                    continue

            # StringBuilder/StringBuffer propagation
            stringbuilder_patterns = [
                rf"(\w+)\.append\s*\(\s*({var_pattern})\s*\)",
                rf"(\w+)\s*=\s*new\s+StringBuilder\s*\(\s*({var_pattern})\s*\)",
            ]
            for pattern in stringbuilder_patterns:
                try:
                    for match in re.finditer(pattern, code, re.IGNORECASE):
                        new_var = match.group(1)
                        if new_var and new_var not in tainted_variables:
                            new_tainted.add(new_var)
                except re.error:
                    continue

            if not new_tainted:
                break
            tainted_variables.update(new_tainted)

    # Step 3: Finally scan sinks, check if there are tainted variables near sink parameters â†’ record taint_flows
    for pattern_tuple in sink_patterns:
        pattern, vuln_type, sink_name = pattern_tuple
        matches = re.finditer(pattern, code, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            start_pos = match.start()
            # Get context of matched line (2 lines before and after)
            line_start = code.rfind("\n", 0, start_pos) + 1
            line_end = code.find("\n", start_pos)
            if line_end == -1:
                line_end = len(code)
            line = code[line_start:line_end]

            # Check if there are tainted variables in this line and surrounding lines
            context_start = max(0, code.rfind("\n", 0, line_start - 1))
            context_end = min(len(code), code.find("\n", line_end + 1))
            context = code[context_start:context_end]

            # Check if there are tainted variables near the sink
            for tainted_var in tainted_variables:
                # Check if variable is used before sink
                var_positions = [
                    m.start()
                    for m in re.finditer(
                        rf"\b{re.escape(tainted_var)}\b", context, re.IGNORECASE
                    )
                ]
                if var_positions and any(
                    pos < start_pos - context_start for pos in var_positions
                ):
                    sinks.append(
                        {
                            "pattern": pattern,
                            "sink_name": sink_name,
                            "vuln_type": vuln_type,
                            "line": line.strip(),
                            "type": "sink",
                        }
                    )
                    taint_flows.append(
                        {
                            "source": tainted_var,
                            "sink": sink_name,
                            "vuln_type": vuln_type,
                            "line": line.strip(),
                        }
                    )
                    break

    return {
        "sources": sources,
        "sinks": sinks,
        "taint_flows": taint_flows,
        "tainted_variables": list(tainted_variables),
    }


def analyze_missing_security(
    before_code: str,
    after_code: str,
    language: str = "java",
    cwe_id: str = None,
) -> Dict:
    """
    Step 2.5: Analyze missing security measures (compare before and after)

    Args:
        before_code: Code before fix
        after_code: Code after fix
        language: Programming language, default 'java'
        cwe_id: CWE ID, used to check relevant security measures, default None (check all)

    Returns:
        Dictionary containing missing security measures:
        - missing_sanitizers: List of missing sanitizers
        - added_security_measures: List of added security measures
    """
    # Check if input is valid
    if not before_code or not isinstance(before_code, str):
        before_code = ""
    if not after_code or not isinstance(after_code, str):
        after_code = ""

    if language.lower() != "java":
        logger.warning(
            f"Currently only supports Java language analysis, skipping {language}"
        )
        return {
            "missing_sanitizers": [],
            "added_security_measures": [],
        }

    # Common Java security measures (sanitizers) - extended with more patterns
    security_patterns = {
        "PreparedStatement": [
            r"PreparedStatement",
            r"prepareStatement\s*\(",
            r"\.setString\s*\(",
            r"\.setInt\s*\(",
            r"\.setLong\s*\(",
            r"\.setBoolean\s*\(",
            r"\.setObject\s*\(",
        ],
        "escapeHtml": [  # CWE-79 (XSS) related
            r"escapeHtml\s*\(",
            r"StringEscapeUtils\.escapeHtml",
            r"Encode\.forHtml",
            r"ESAPI\.encoder\s*\(\)\.encodeForHTML",
            r"HtmlUtils\.htmlEscape",
            r"OWASP\.ESAPI\.encoder\(\)\.encodeForHTML",
            r"\.escapeHtml",
            r"HtmlEscape",
            r"XSSUtils\.escape",
        ],
        "normalize": [  # CWE-22 (Path Traversal) related
            r"Paths\.normalize\s*\(",
            r"\.toRealPath\s*\(",
            r"\.getCanonicalPath\s*\(",
            r"\.getCanonicalFile\s*\(",
            r"\.toPath\s*\(\)\.normalize",
            r"\.normalize\s*\(",
        ],
        "pathValidation": [  # CWE-22 (Path Traversal) related
            r"\.startsWith\s*\(",
            r"\.contains\s*\(",
            r"\.equals\s*\(",
            r"\.indexOf\s*\(",
            r"\.matches\s*\(",
            r"PathValidator",
            r"isValidPath",
            r"isAllowedPath",
        ],
        "ObjectInputFilter": [
            r"ObjectInputFilter",
            r"\.setObjectInputFilter\s*\(",
            r"ObjectInputFilter\.Config\.createFilter",
        ],
        "validation": [
            r"Pattern\.compile",
            r"\.matches\s*\(",
            r"StringUtils\.isNotBlank",
            r"StringUtils\.isNotEmpty",
            r"Validator\.validate",
            r"@Valid\s+",
            r"@NotNull\s+",
            r"@NotBlank\s+",
        ],
        "whitelist": [
            r"whitelist",
            r"allowlist",
            r"isAllowed",
            r"isWhitelisted",
        ],
        "sanitize": [
            r"sanitize\s*\(",
            r"Sanitizer\.",
            r"clean\s*\(",
        ],
    }

    missing_sanitizers = []
    added_security_measures = []

    # Check which security measures are missing in before_code
    for measure_name, patterns in security_patterns.items():
        found_in_before = False
        found_in_after = False

        for pattern in patterns:
            if re.search(pattern, before_code, re.IGNORECASE):
                found_in_before = True
                break
            if re.search(pattern, after_code, re.IGNORECASE):
                found_in_after = True

        if not found_in_before and found_in_after:
            missing_sanitizers.append(measure_name)
            added_security_measures.append(measure_name)

    return {
        "missing_sanitizers": missing_sanitizers,
        "added_security_measures": added_security_measures,
    }


def extract_vulnerability_pattern(
    method_code: str,
    file_code_before: str,
    file_code_after: str,
    language: str = "java",
    cwe_id: str = None,
) -> Dict:
    """
    Step 2.6: Extract vulnerability patterns (source/sink/taint/missing security)

    Args:
        method_code: Method-level code (before_change)
        file_code_before: File-level code before fix
        file_code_after: File-level code after fix
        language: Programming language, default 'java'
        cwe_id: CWE ID, used for targeted analysis of specific vulnerability types, supports:
                - CWE-79: Cross-site Scripting (XSS)
                - CWE-22: Path Traversal
                - NVD-CWE-noinfo: Insufficient Information (uses generic patterns)
                Default None (uses generic patterns)

    Returns:
        Dictionary containing vulnerability patterns:
        - sources: List of sources
        - sinks: List of sinks
        - taint_flows: List of taint flows
        - missing_sanitizers: List of missing sanitizers
        - pattern_summary: Pattern summary
    """
    # Check if method_code is valid
    if not method_code or not isinstance(method_code, str) or method_code.strip() == "":
        # Return empty pattern
        return {
            "sources": [],
            "sinks": [],
            "taint_flows": [],
            "tainted_variables": [],
            "missing_sanitizers": [],
            "added_security_measures": [],
            "pattern_summary": {
                "has_source": False,
                "has_sink": False,
                "has_taint_flow": False,
                "missing_security": False,
            },
        }

    # Analyze source/sink/taint (filtered by CWE type)
    taint_analysis = analyze_source_sink_taint(method_code, language, cwe_id)

    # Analyze missing security measures (check relevant measures based on CWE type)
    security_analysis = analyze_missing_security(
        file_code_before if file_code_before else method_code,
        file_code_after if file_code_after else "",
        language,
        cwe_id,
    )

    # Generate pattern summary
    pattern_summary = {
        "has_source": len(taint_analysis["sources"]) > 0,
        "has_sink": len(taint_analysis["sinks"]) > 0,
        "has_taint_flow": len(taint_analysis["taint_flows"]) > 0,
        "missing_security": len(security_analysis["missing_sanitizers"]) > 0,
    }

    return {
        "sources": taint_analysis["sources"],
        "sinks": taint_analysis["sinks"],
        "taint_flows": taint_analysis["taint_flows"],
        "tainted_variables": taint_analysis["tainted_variables"],
        "missing_sanitizers": security_analysis["missing_sanitizers"],
        "added_security_measures": security_analysis["added_security_measures"],
        "pattern_summary": pattern_summary,
    }


def extract_api_names_from_code(code: str, language: str = "java") -> List[str]:
    """
    Extract real API names (class names, library names, core API names) from code snippet.

    This follows the ASIA CCS paper Step A approach: extract stable library/API names from code.
    The paper manually selects a few stable library/API names from a vulnerability snippet,
    focusing on:
    - Library function names (e.g., getParameter, readFile, createServer)
    - Module/class names (e.g., HttpServletRequest, Files, Paths, Runtime)
    - Core API names (e.g., URL, File, ProcessBuilder)

    We do NOT extract:
    - Variable names (unstable, vary by implementation)
    - Function parameter names (unstable)
    - Abstract type labels (e.g., "http", "fileParam")

    Args:
        code: Code snippet to analyze
        language: Programming language, default 'java'

    Returns:
        List of extracted API names (e.g., ['HttpServletRequest', 'Files', 'Paths', 'Runtime', 'getParameter'])
        Limited to top 5 most relevant names (following paper's ~5 primary keywords approach)
    """
    if not code or not isinstance(code, str) or code.strip() == "":
        return []

    if language.lower() != "java":
        return []

    api_names = set()

    # Common Java library/class patterns to extract
    # These are stable API names that appear in code, not variable names or parameters

    # 1. Common Java standard library classes
    standard_classes = [
        r"\bFiles\b",
        r"\bPaths\b",
        r"\bPath\b",
        r"\bRuntime\b",
        r"\bProcessBuilder\b",
        r"\bURL\b",
        r"\bURI\b",
        r"\bFile\b",
        r"\bFileInputStream\b",
        r"\bFileOutputStream\b",
        r"\bBufferedReader\b",
        r"\bPrintWriter\b",
        r"\bStringBuilder\b",
        r"\bStringBuffer\b",
    ]

    # 2. Servlet/Jakarta EE API classes
    servlet_classes = [
        r"\bHttpServletRequest\b",
        r"\bHttpServletResponse\b",
        r"\bServletRequest\b",
        r"\bServletResponse\b",
        r"\bHttpSession\b",
        r"\bCookie\b",
    ]

    # 3. Spring Framework classes
    spring_classes = [
        r"\bRequestParam\b",
        r"\bPathVariable\b",
        r"\bRequestHeader\b",
        r"\bRequestBody\b",
    ]

    # 4. Database/ORM classes
    db_classes = [
        r"\bPreparedStatement\b",
        r"\bStatement\b",
        r"\bConnection\b",
        r"\bResultSet\b",
    ]

    # 5. Security/Validation classes
    security_classes = [
        r"\bStringEscapeUtils\b",
        r"\bESAPI\b",
        r"\bObjectInputFilter\b",
    ]

    # 6. Common method calls that represent library APIs (extract the method name)
    # e.g., "request.getParameter" -> extract "getParameter"
    method_call_patterns = [
        (r"\.getParameter\s*\(", "getParameter"),
        (r"\.getParameterValues\s*\(", "getParameterValues"),
        (r"\.getParameterMap\s*\(", "getParameterMap"),
        (r"\.getHeader\s*\(", "getHeader"),
        (r"\.getHeaders\s*\(", "getHeaders"),
        (r"\.getCookies\s*\(", "getCookies"),
        (r"\.getQueryString\s*\(", "getQueryString"),
        (r"\.getWriter\s*\(", "getWriter"),
        (r"\.readObject\s*\(", "readObject"),
        (r"\.createServer\s*\(", "createServer"),
        (r"\.read\s*\(", "read"),
        (r"\.readAllBytes\s*\(", "readAllBytes"),
        (r"\.exec\s*\(", "exec"),
        (r"\.execute\s*\(", "execute"),
        (r"\.executeQuery\s*\(", "executeQuery"),
        (r"\.executeUpdate\s*\(", "executeUpdate"),
        (r"createStatement\s*\(", "createStatement"),
    ]

    # Extract standard classes
    for pattern in (
        standard_classes
        + servlet_classes
        + spring_classes
        + db_classes
        + security_classes
    ):
        matches = re.findall(pattern, code)
        for match in matches:
            if match and len(match) > 2:  # Filter out very short matches
                api_names.add(match)

    # Extract method names from common API calls
    for pattern, method_name in method_call_patterns:
        if re.search(pattern, code):
            api_names.add(method_name)

    # Also extract from import statements (more reliable)
    import_pattern = r"import\s+(?:static\s+)?(?:[\w.]+\.)?([A-Z]\w+)(?:\.\*)?;"
    import_matches = re.findall(import_pattern, code)
    for match in import_matches:
        if match and len(match) > 2:
            api_names.add(match)

    # Filter out common variable names and keep only API-like names
    # API names are typically: PascalCase, or common library names
    filtered_names = []
    common_vars = {
        "string",
        "object",
        "list",
        "map",
        "set",
        "array",
        "int",
        "long",
        "double",
        "float",
        "boolean",
        "char",
        "byte",
    }

    for name in api_names:
        # Skip if it's a common variable name
        if name.lower() in common_vars:
            continue
        # Keep if it's PascalCase (likely a class) or a known API name
        if name[0].isupper() or name in ["URL", "URI", "fs", "http"]:
            filtered_names.append(name)

    # Return top 5 most relevant API names (following paper's approach of ~5 primary keywords)
    return filtered_names[:5]


def generate_pattern_key(pattern_dict: Dict, cwe_id: str = None) -> str:
    """
    Generate pattern key (pattern_key) to aggregate similar vulnerability instances into a pattern class

    Note: This function uses a coarse-grained aggregation approach (Solution 1):
    - Only considers cwe_id, sink_types, and source_types
    - Ignores missing_sanitizers to allow better aggregation of similar patterns
    - This is suitable for GitHub search queries where source and sink are the primary focus

    Args:
        pattern_dict: Pattern dictionary returned by extract_vulnerability_pattern
        cwe_id: CWE ID, if provided use it, otherwise try to extract from pattern_dict

    Returns:
        pattern_key: Pattern key string, format: "cwe_id|sink_types|source_types"
    """
    if cwe_id is None:
        cwe_id = pattern_dict.get("pattern_summary", {}).get("cwe_id", "unknown")
    if not cwe_id or cwe_id == "unknown":
        cwe_id = "unknown"

    # Extract sink types (deduplicate and sort)
    sinks = pattern_dict.get("sinks", [])
    sink_types = sorted(
        set([s.get("sink_name", "") for s in sinks if isinstance(s, dict)])
    )
    sink_key = ",".join(sink_types) if sink_types else "unknown"

    # Extract source types (deduplicate and sort)
    sources = pattern_dict.get("sources", [])
    source_types = sorted(
        set([s.get("type", "") for s in sources if isinstance(s, dict)])
    )
    source_key = ",".join(source_types) if source_types else "unknown"

    # Solution 1: Ignore missing_sanitizers for coarse-grained aggregation
    # This allows similar patterns with different security measures to be grouped together
    # Combine into pattern_key (without missing_sanitizers)
    pattern_key = f"{cwe_id}|{sink_key}|{source_key}"
    return pattern_key


def aggregate_patterns(
    patterns_df: pd.DataFrame, output_dir: Optional[Path] = None
) -> pd.DataFrame:
    """
    Aggregate vulnerability pattern instances into pattern classes (pattern class)

    Following ASIA CCS paper Step A approach for primary keywords generation:
    - Extract stable library/API names from code snippets (not abstract labels)
    - Use real API method names (e.g., getParameter, execute, FileInputStream)
    - Do NOT use abstract type labels (e.g., "http", "fileParam")
    - Do NOT use "missing_XXX" keywords (these are not real API names in code)
    - Limit to ~5-7 primary keywords per pattern

    Args:
        patterns_df: DataFrame containing all pattern instances
        output_dir: Output directory for saving CSV files. If None, no files will be saved.

    Returns:
        Aggregated DataFrame containing statistical information for each pattern class
    """
    if patterns_df.empty:
        return pd.DataFrame()

    # Generate pattern_key for each instance
    pattern_keys = []
    for _, row in patterns_df.iterrows():
        pattern_dict = row.get("pattern_dict", {})
        cwe_id = row.get("cwe_id", "unknown")
        if isinstance(pattern_dict, str):
            try:
                import ast

                pattern_dict = ast.literal_eval(pattern_dict)
            except:
                pattern_dict = {}
        pattern_key = generate_pattern_key(pattern_dict, cwe_id)
        pattern_keys.append(pattern_key)

    patterns_df = patterns_df.copy()
    patterns_df["pattern_key"] = pattern_keys

    # Aggregate by pattern_key
    aggregated = []

    for pattern_key, group in patterns_df.groupby("pattern_key"):
        # Statistical information
        instance_count = len(group)  # Number of instances
        cve_count = group["cve_id"].nunique()  # Number of CVEs covered
        repo_count = (
            group["repo_url"].nunique() if "repo_url" in group.columns else 0
        )  # Number of repos covered

        # Extract representative information (take first instance)
        first_row = group.iloc[0]

        # Parse pattern_key to get components
        # Note: pattern_key format is now "cwe_id|sink_types|source_types" (without missing_sanitizers)
        parts = pattern_key.split("|")
        cwe_id = parts[0] if len(parts) > 0 else "unknown"
        sink_types = parts[1].split(",") if len(parts) > 1 and parts[1] else []
        source_types = parts[2].split(",") if len(parts) > 2 and parts[2] else []
        # missing_sanitizers is no longer part of pattern_key, but we can still extract it from pattern_dict
        missing_sanitizers = []
        pattern_dict = first_row.get("pattern_dict", {})
        if isinstance(pattern_dict, str):
            try:
                import ast

                pattern_dict = ast.literal_eval(pattern_dict)
            except:
                pattern_dict = {}
        if isinstance(pattern_dict, dict):
            missing_sanitizers_list = pattern_dict.get("missing_sanitizers", [])
            if isinstance(missing_sanitizers_list, list):
                missing_sanitizers = missing_sanitizers_list
            elif isinstance(missing_sanitizers_list, str):
                try:
                    import ast

                    missing_sanitizers = ast.literal_eval(missing_sanitizers_list)
                except:
                    missing_sanitizers = []

        # Generate primary keywords following ASIA CCS paper Step A approach:
        # Extract stable library/API names from code snippet, not abstract labels or "missing_xxx"
        keywords = []

        # 1. Use sink_types as primary keywords (these are real API names like execute, println, FileInputStream)
        keywords.extend([f"{sink}" for sink in sink_types if sink])

        # 2. Extract real API names from representative method code
        representative_method_code = first_row.get("method_code", "")
        if representative_method_code:
            # Extract real API names (class names, library names) from code
            api_names = extract_api_names_from_code(
                representative_method_code, language="java"
            )
            keywords.extend(api_names)

        # 3. Extract source method names from pattern_dict (real API calls like getParameter, getHeader)
        # These are actual method names that appear in code, not abstract type labels
        representative_pattern = first_row.get("pattern_dict", {})
        if isinstance(representative_pattern, str):
            try:
                import ast

                representative_pattern = ast.literal_eval(representative_pattern)
            except:
                representative_pattern = {}

        sources = representative_pattern.get("sources", [])
        if isinstance(sources, str):
            try:
                import ast

                sources = ast.literal_eval(sources)
            except:
                sources = []

        # Extract actual source method names (e.g., getParameter, getHeader, readObject)
        # These are real API calls, not abstract type labels like "http" or "fileParam"
        # The source_type in pattern_dict is the actual method name from JAVA_SOURCE_PATTERNS
        real_api_methods = {
            "getParameter",
            "getParameterValues",
            "getParameterMap",
            "getHeader",
            "getHeaders",
            "getCookies",
            "getQueryString",
            "getRequestURI",
            "getPathInfo",
            "getServletPath",
            "getAttribute",
            "readObject",
            "getenv",
            "getProperty",
            "readAllBytes",
            "read",
            "createServer",
            "getInputStream",
            "getReader",
        }

        for source in sources:
            if isinstance(source, dict):
                source_type = source.get("type", "")
                # If source_type is a real API method name, use it
                # Skip abstract category labels like "http", "file", "fileParam", "env"
                if source_type and source_type in real_api_methods:
                    keywords.append(source_type)

        # Remove duplicates and filter out empty strings
        primary_keywords = [k for k in list(set(keywords)) if k and k.strip()]

        # Limit to top 5-7 keywords (following paper's approach of ~5 primary keywords)
        primary_keywords = primary_keywords[:7]

        # Generate structure template (extracted from representative instance)
        representative_pattern = first_row.get("pattern_dict", {})
        if isinstance(representative_pattern, str):
            try:
                import ast

                representative_pattern = ast.literal_eval(representative_pattern)
            except:
                representative_pattern = {}

        # Generate commit links from hash and repo_url
        commit_links = []
        if "hash" in group.columns and "repo_url" in group.columns:
            for _, row in group.iterrows():
                hash_val = row.get("hash", "")
                repo_url = row.get("repo_url", "")
                if hash_val and repo_url:
                    # Generate GitHub commit link
                    # Format: https://github.com/owner/repo/commit/hash
                    if "github.com" in repo_url:
                        # Remove trailing .git if present
                        repo_url_clean = repo_url.rstrip(".git")
                        commit_link = f"{repo_url_clean}/commit/{hash_val}"
                        commit_links.append(commit_link)

        # Get unique commit hashes and links
        unique_hashes = (
            sorted(group["hash"].unique()) if "hash" in group.columns else []
        )
        unique_commit_links = sorted(list(set(commit_links))) if commit_links else []

        aggregated.append(
            {
                "pattern_key": pattern_key,
                "cwe_id": cwe_id,
                "cwe_name": first_row.get("cwe_name", ""),
                "sink_types": ",".join(sink_types),
                "source_types": ",".join(source_types),
                "missing_sanitizers": ",".join(missing_sanitizers),
                "instance_count": instance_count,
                "cve_count": cve_count,
                "repo_count": repo_count,
                "primary_keywords": ",".join(primary_keywords),
                "representative_pattern": representative_pattern,
                "representative_method_code": first_row.get("method_code", ""),
                "all_cve_ids": ",".join(sorted(group["cve_id"].unique())),
                "all_repo_urls": (
                    ",".join(sorted(group["repo_url"].unique()))
                    if "repo_url" in group.columns
                    else ""
                ),
                "all_commit_hashes": ",".join(unique_hashes) if unique_hashes else "",
                "all_commit_links": (
                    " | ".join(unique_commit_links) if unique_commit_links else ""
                ),
            }
        )

    aggregated_df = pd.DataFrame(aggregated)

    # Sort by instance count in descending order
    aggregated_df = aggregated_df.sort_values("instance_count", ascending=False)

    return aggregated_df


def process_cwe_based_patterns(
    vulnerable_code_df: pd.DataFrame,
    db_connector: DatabaseConnector,
    top_n: int = 3,
    min_score: int = 65,
    programming_languages: List[str] = None,
    output_dir: Path = None,
) -> pd.DataFrame:
    """
    New pattern recognition logic: Based on CWE frequency statistics and source/sink/taint analysis from vulnerable_code_df

    Args:
        vulnerable_code_df: DataFrame containing vulnerable code (must include cve_id column)
                           Note: If this DataFrame has been filtered by extract_java_vulnerable_code's
                           top_n parameter, it only contains data for top n CWE
        db_connector: Database connector
        top_n: Return top n most common CWE, default 3
               (If vulnerable_code_df has been filtered, calculate based on filtered data)
        min_score: Minimum value of fixes.score, default 65 (used for querying method changes)
        programming_languages: List of programming languages, default ['Java']
        output_dir: Output directory, default None (use output directory under current directory)

    Returns:
        DataFrame containing pattern records
    """
    if programming_languages is None:
        programming_languages = ["Java"]

    if output_dir is None:
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)

    logger.info("\n" + "=" * 60)
    logger.info("Starting CWE-based pattern recognition (based on vulnerable_code_df)")
    logger.info("=" * 60)

    # Check if input DataFrame is empty
    if vulnerable_code_df.empty:
        logger.warning(
            "vulnerable_code_df is empty, cannot perform pattern recognition"
        )
        return pd.DataFrame()

    # Check if required columns exist
    if "cve_id" not in vulnerable_code_df.columns:
        logger.error("vulnerable_code_df missing 'cve_id' column")
        return pd.DataFrame()

    # Step 2.1: Get CWE classification information (get once to avoid repeated queries)
    cwe_classification_df = _get_cwe_classification(vulnerable_code_df, db_connector)
    if cwe_classification_df.empty:
        logger.warning("No CWE classification information found")
        return pd.DataFrame()

    # Step 2.2: Get all CWE from vulnerable_code_df (vulnerable_code_df already contains top n CWE data)
    # Count CVE numbers for each CWE, for log output
    top_cwe_df = (
        cwe_classification_df[
            cwe_classification_df["cve_id"].isin(vulnerable_code_df["cve_id"])
        ]
        .groupby(["cwe_id", "cwe_name"])
        .size()
        .reset_index(name="fix_count")
        .query("fix_count > 1")
        .sort_values("fix_count", ascending=False)
    )

    if top_cwe_df.empty:
        logger.warning("No matching CWE found")
        return pd.DataFrame()

    logger.info(f"Found {len(top_cwe_df)} CWE (from filtered vulnerable_code_df):")
    for _, row in top_cwe_df.iterrows():
        logger.info(
            f"  {row['cwe_id']}: {row['cwe_name']} (fix_count: {row['fix_count']})"
        )

    # Output top_cwe_df's cwe_id and cwe_name to file
    top_cwe_output_file = output_dir / f"top_cwe_top{top_n}.csv"
    top_cwe_df[["cwe_id", "cwe_name"]].to_csv(
        top_cwe_output_file, index=False, encoding="utf-8"
    )
    logger.info(f"Top CWE list saved to: {top_cwe_output_file}")

    all_patterns = []
    if not programming_languages or len(programming_languages) == 0:
        logger.error("programming_languages list is empty")
        return pd.DataFrame()
    language = programming_languages[0].lower()

    # Process each CWE
    for _, cwe_row in top_cwe_df.iterrows():
        cwe_id, cwe_name = cwe_row["cwe_id"], cwe_row["cwe_name"]
        logger.info(f"\nProcessing CWE: {cwe_id} - {cwe_name}")

        # Step 2.3: Get data for this CWE from vulnerable_code_df (includes file_change_id)
        cve_df = get_cves_by_cwe_from_df(
            vulnerable_code_df, cwe_id, cwe_classification_df
        )

        if cve_df.empty:
            logger.warning(f"No matching CVEs found for CWE {cwe_id}")
            continue

        # Iterate through cve_df, analyze each file_change_id
        for _, cve_row in cve_df.iterrows():
            file_change_id = cve_row.get("file_change_id")

            if not file_change_id:
                logger.warning(f"  file_change_id is empty, skipping")
                continue

            logger.info(f"  Processing file_change_id: {file_change_id}")

            # Step 2.4: Get method-level code changes based on file_change_id
            method_changes_df = get_method_changes_for_file_changes(
                db_connector, file_change_id, programming_languages
            )

            if method_changes_df.empty:
                logger.warning(
                    f"  file_change_id={file_change_id} has no method-level change records"
                )
                continue

            # Analyze each method change
            for _, method_row in method_changes_df.iterrows():
                file_change_id = method_row.get("file_change_id")
                method_change_id = method_row.get("method_change_id")
                before_change = method_row.get("before_change", "")
                start_line = method_row.get("start_line")
                end_line = method_row.get("end_line")

                file_code_before = method_row.get("file_code_before", "")

                # Convert start_line and end_line to integers
                try:
                    start_line = int(start_line) if start_line else None
                    end_line = int(end_line) if end_line else None
                except (ValueError, TypeError):
                    continue

                # Extract method code from file code
                method_code = extract_method_code_from_file(
                    file_code_before, start_line, end_line
                )

                if not method_code:
                    logger.warning(
                        f"  Unable to extract method code from file code: file_change_id={file_change_id}, "
                        f"method_change_id={method_change_id}, start_line={start_line}, "
                        f"end_line={end_line}, before_change={before_change}"
                    )
                    continue

                # Step 2.5-2.6: Extract vulnerability patterns (pass CWE ID)
                pattern = extract_vulnerability_pattern(
                    method_code,
                    file_code_before,
                    method_row.get("file_code_after", ""),
                    language,
                    cwe_id,  # Pass CWE ID to support targeted vulnerability type analysis
                )

                # Adjust filter conditions based on CWE type
                summary = pattern["pattern_summary"]

                # CWE-79 (XSS) and CWE-22 (Path Traversal): require source, sink, taint flow and missing_security
                # NVD-CWE-noinfo: Relax conditions, only need source and sink (due to insufficient information)
                if cwe_id == "NVD-CWE-noinfo":
                    # For CWE with insufficient information, relax conditions: only need source and sink
                    if not (summary["has_source"] and summary["has_sink"]):
                        continue
                else:
                    # For specific CWE types, require complete taint flow and missing security measures
                    # This can exclude code that already uses security APIs (e.g., PreparedStatement)
                    if not (
                        summary["has_source"]
                        and summary["has_sink"]
                        and summary["has_taint_flow"]
                        and summary[
                            "missing_security"
                        ]  # Require missing security measures, exclude secure usage
                    ):
                        continue

                all_patterns.append(
                    {
                        "cwe_id": cwe_id,
                        "cwe_name": cwe_name,
                        "cve_id": method_row["cve_id"],
                        "hash": method_row.get("hash", ""),  # Commit hash
                        "repo_url": method_row.get("repo_url", "")
                        or cve_row.get("repo_url", ""),  # Repository URL
                        "file_change_id": method_row["file_change_id"],
                        "method_change_id": method_row["method_change_id"],
                        "method_name": method_row["method_name"],
                        "signature": method_row["signature"],
                        "sources": str(pattern["sources"]),
                        "sinks": str(pattern["sinks"]),
                        "taint_flows": str(pattern["taint_flows"]),
                        "tainted_variables": str(pattern["tainted_variables"]),
                        "missing_sanitizers": str(pattern["missing_sanitizers"]),
                        "added_security_measures": str(
                            pattern["added_security_measures"]
                        ),
                        "pattern_dict": pattern,  # Save pattern dictionary for subsequent GitHub query generation
                        "method_code": method_code,  # Save complete method code
                    }
                )

    if not all_patterns:
        logger.warning("No matching vulnerability patterns found")
        return pd.DataFrame()

    patterns_df = pd.DataFrame(all_patterns)
    logger.info(f"Found {len(patterns_df)} vulnerability pattern instances in total")

    # Step 2.7: Pattern aggregation (aggregate instances into pattern class)
    logger.info("\nStep 2.7: Starting pattern aggregation...")
    aggregated_patterns_df = aggregate_patterns(patterns_df, output_dir=output_dir)

    if aggregated_patterns_df.empty:
        logger.warning("No pattern classes found after aggregation")
        return pd.DataFrame()

    logger.info(
        f"Found {len(aggregated_patterns_df)} pattern classes after aggregation"
    )
    logger.info("\nTop-10 pattern classes (sorted by instance count):")
    for idx, (_, row) in enumerate(aggregated_patterns_df.head(10).iterrows(), 1):
        logger.info(
            f"  {idx}. {row['pattern_key']}: "
            f"instance_count={row['instance_count']}, "
            f"cve_count={row['cve_count']}, "
            f"repo_count={row['repo_count']}"
        )

    # Save aggregated pattern classes
    aggregated_output_file = output_dir / "aggregated_patterns.csv"
    # Exclude columns that cannot be directly serialized
    columns_to_save = [
        col
        for col in aggregated_patterns_df.columns
        if col not in ["representative_pattern"]
    ]
    aggregated_patterns_df[columns_to_save].to_csv(
        aggregated_output_file, index=False, encoding="utf-8"
    )
    logger.info(f"Aggregated pattern classes saved to: {aggregated_output_file}")

    # Also save original instance data (for subsequent analysis)
    instances_output_file = output_dir / "pattern_instances.csv"
    # Exclude columns that cannot be directly serialized
    instance_columns_to_save = [
        col for col in patterns_df.columns if col not in ["pattern_dict"]
    ]
    patterns_df[instance_columns_to_save].to_csv(
        instances_output_file, index=False, encoding="utf-8"
    )
    logger.info(f"Pattern instance data saved to: {instances_output_file}")

    # Return aggregated pattern class DataFrame
    return aggregated_patterns_df


def main(
    top_n: int = 3,
    min_score: int = 65,
    exclude_merge_commits: bool = True,
    programming_languages: List[str] = None,
    require_diff: bool = True,
):
    """
    Main function: Extract candidate recurring vulnerability code patterns

    Args:
        top_n: Return top n patterns by occurrence count, default 3
        min_score: Minimum value of fixes.score, default 65
        exclude_merge_commits: Whether to exclude merge commits, default True
        programming_languages: List of programming languages, default ['Java']
        require_diff: Whether to require non-empty diff, default True
    """
    if programming_languages is None:
        programming_languages = ["Java"]

    logger.info("=" * 60)
    logger.info("Starting to extract candidate recurring vulnerability code patterns")
    logger.info(f"Configuration: top_n={top_n}, min_score={min_score}")
    logger.info("=" * 60)

    # Set output directory
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)

    # Initialize database connection
    db_connector = DatabaseConnector()

    # Step 1: Filter vulnerable code based on conditions (includes top n CWE filtering)
    logger.info("\nStep 1: Extract vulnerable code (includes top n CWE filtering)")
    vulnerable_code_df = extract_java_vulnerable_code(
        db_connector,
        min_score=min_score,
        exclude_merge_commits=exclude_merge_commits,
        programming_languages=programming_languages,
        require_diff=require_diff,
        top_n=top_n,
        output_dir=output_dir,
    )

    # Check if extracted data is empty
    if vulnerable_code_df.empty:
        logger.warning("No vulnerable code extracted, program terminated")
        return

    # Step 2: Identify recurring patterns (using new CWE-based method)
    # Note: vulnerable_code_df already contains top n CWE data, so use it directly here
    logger.info("\nStep 2: Using CWE-based pattern recognition method")
    logger.info("Note: vulnerable_code_df has been filtered to top n CWE data")
    recurring_patterns_df = process_cwe_based_patterns(
        vulnerable_code_df,
        db_connector,
        top_n=top_n,
        min_score=min_score,
        programming_languages=programming_languages,
        output_dir=output_dir,
    )

    if len(recurring_patterns_df) <= 0:
        logger.warning("No patterns found")
        # Even if no patterns found, print statistics
        logger.info("\n" + "=" * 60)
        logger.info("Statistics:")
        logger.info(f"  Total records: {len(vulnerable_code_df)}")
        logger.info(f"  Unique CVE count: {vulnerable_code_df['cve_id'].nunique()}")
        logger.info(f"  Unique commit count: {vulnerable_code_df['hash'].nunique()}")
        logger.info(
            f"  Unique repository count: {vulnerable_code_df['repo_url'].nunique()}"
        )
        logger.info(f"  Unique file count: {vulnerable_code_df['filename'].nunique()}")
        logger.info(f"  Identified recurring patterns: 0")
        logger.info("=" * 60)
        return

    # Step 3: Generate GitHub search queries
    logger.info(
        f"\nStep 3: Generate GitHub queries for {len(recurring_patterns_df)} patterns"
    )
    query_generator = GitHubQueryGenerator()
    # Rename representative_pattern to pattern_dict for GitHub query generation
    if "representative_pattern" in recurring_patterns_df.columns:
        recurring_patterns_df = recurring_patterns_df.rename(
            columns={"representative_pattern": "pattern_dict"}
        )
    recurring_patterns_df = query_generator.generate_github_search_keywords(
        recurring_patterns_df,
        output_dir=output_dir,
        top_n=top_n,
        save_file=True,
    )

    # Print statistics
    logger.info("\n" + "=" * 60)
    logger.info("Statistics:")
    logger.info(f"  Total records: {len(vulnerable_code_df)}")
    logger.info(f"  Unique CVE count: {vulnerable_code_df['cve_id'].nunique()}")
    logger.info(f"  Unique commit count: {vulnerable_code_df['hash'].nunique()}")
    logger.info(
        f"  Unique repository count: {vulnerable_code_df['repo_url'].nunique()}"
    )
    logger.info(f"  Unique file count: {vulnerable_code_df['filename'].nunique()}")
    logger.info(f"  Identified recurring patterns: {len(recurring_patterns_df)}")
    if len(recurring_patterns_df) > 0:
        logger.info(f"  Generated GitHub queries: {len(recurring_patterns_df)}")
    logger.info("=" * 60)

    logger.info("\nExtraction completed!")


def parse_arguments():
    """
    Parse command line arguments

    Returns:
        argparse.Namespace: Parsed command line arguments object
    """
    parser = argparse.ArgumentParser(
        description="Extract candidate recurring vulnerability code patterns"
    )
    parser.add_argument(
        "--top-n",
        type=int,
        default=3,
        help="Return top n patterns by occurrence count (default: 3)",
    )
    parser.add_argument(
        "--min-score",
        type=int,
        default=65,
        help="Minimum value of fixes.score (default: 65)",
    )
    parser.add_argument(
        "--include-merge",
        action="store_true",
        help="Include merge commits (default: excluded)",
    )
    parser.add_argument(
        "--languages",
        nargs="+",
        default=["java"],  # Can use any case form, e.g., "java", "JAVA", "Java"
        help="List of programming languages, case-insensitive (default: java). Example: --languages java or --languages Java Go",
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_arguments()

    main(
        top_n=args.top_n,
        min_score=args.min_score,
        exclude_merge_commits=not args.include_merge,
        programming_languages=args.languages,
    )
